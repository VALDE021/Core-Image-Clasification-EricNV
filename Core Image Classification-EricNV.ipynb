{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42d59875-10ee-4dd5-8692-f7fe441b5547",
   "metadata": {},
   "source": [
    "# <u><center> Image Classification (Core)\n",
    "* Authored by: Eric N. Valdez\n",
    "* Date: 03-22-2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fcf5bb-3305-4f64-8604-a62ef105c3b2",
   "metadata": {},
   "source": [
    "# Assignment:\n",
    "- ## For this assignment, you will classify chest X-rays as \"normal,\" \"pneumonia,\" or \"covid\" using Convolutional Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b73741c-fdb2-4aef-a93b-4b6e68cd0200",
   "metadata": {},
   "source": [
    "# <u>Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55792b71-0dc4-4e8f-b8b6-ea7040da44fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# Set the seed for NumPy\n",
    "np.random.seed(42)\n",
    "# Set the seed for TensorFlow\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "import os, glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import load_img, img_to_array, array_to_img\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import visualkeras as vk\n",
    "\n",
    "import keras_tuner as kt\n",
    "from keras_tuner import HyperParameters as hp\n",
    "\n",
    "import os\n",
    "\n",
    "folder = 'KerasTuner/'\n",
    "os.makedirs(folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caa6105-d3dd-40a2-bd62-d8b88bea780e",
   "metadata": {},
   "source": [
    "# <u>Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "427d8486-df94-4a1b-b04f-f51f47006b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_pred_labels(model,ds):\n",
    "    \"\"\"Gets the labels and predicted probabilities from a Tensorflow model and Dataset object.\n",
    "    Adapted from source: https://stackoverflow.com/questions/66386561/keras-classification-report-accuracy-is-different-between-model-predict-accurac\n",
    "    \"\"\"\n",
    "    y_true = []\n",
    "    y_pred_probs = []\n",
    "    \n",
    "    # Loop through the dataset as a numpy iterator\n",
    "    for images, labels in ds.as_numpy_iterator():\n",
    "        \n",
    "        # Get prediction with batch_size=1\n",
    "        y_probs = model.predict(images, batch_size=1, verbose=0)\n",
    "        # Combine previous labels/preds with new labels/preds\n",
    "        y_true.extend(labels)\n",
    "        y_pred_probs.extend(y_probs)\n",
    "    ## Convert the lists to arrays\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred_probs = np.array(y_pred_probs)\n",
    "    \n",
    "    return y_true, y_pred_probs\n",
    "def convert_y_to_sklearn_classes(y, verbose=False):\n",
    "    # If already one-dimension\n",
    "    if np.ndim(y)==1:\n",
    "        if verbose:\n",
    "            print(\"- y is 1D, using it as-is.\")\n",
    "        return y\n",
    "        \n",
    "    # If 2 dimensions with more than 1 column:\n",
    "    elif y.shape[1]>1:\n",
    "        if verbose:\n",
    "            print(\"- y is 2D with >1 column. Using argmax for metrics.\")   \n",
    "        return np.argmax(y, axis=1)\n",
    "    \n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"y is 2D with 1 column. Using round for metrics.\")\n",
    "        return np.round(y).flatten().astype(int)\n",
    "## PREVIOUS CLASSIFICATION_METRICS FUNCTION FROM INTRO TO ML\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def classification_metrics(y_true, y_pred, label='',\n",
    "                           output_dict=False, figsize=(8,4),\n",
    "                           normalize='true', cmap='Blues',\n",
    "                           colorbar=False,values_format=\".2f\"):\n",
    "    \"\"\"Modified version of classification metrics function from Intro to Machine Learning.\n",
    "    Updates:\n",
    "    - Reversed raw counts confusion matrix cmap  (so darker==more).\n",
    "    - Added arg for normalized confusion matrix values_format\n",
    "    \"\"\"\n",
    "    # Get the classification report\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    \n",
    "    ## Print header and report\n",
    "    header = \"-\"*70\n",
    "    print(header, f\" Classification Metrics: {label}\", header, sep='\\n')\n",
    "    print(report)\n",
    "    \n",
    "    ## CONFUSION MATRICES SUBPLOTS\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=figsize)\n",
    "    \n",
    "    # Create a confusion matrix  of raw counts (left subplot)\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred,\n",
    "                                            normalize=None, \n",
    "                                            cmap='gist_gray_r',# Updated cmap\n",
    "                                            values_format=\"d\", \n",
    "                                            colorbar=colorbar,\n",
    "                                            ax = axes[0]);\n",
    "    axes[0].set_title(\"Raw Counts\")\n",
    "\n",
    "    \n",
    "    # Create a confusion matrix with the data with normalize argument \n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred,\n",
    "                                            normalize=normalize,\n",
    "                                            cmap=cmap, \n",
    "                                            values_format=values_format, #New arg\n",
    "                                            colorbar=colorbar,\n",
    "                                            ax = axes[1]);\n",
    "    axes[1].set_title(\"Normalized Confusion Matrix\")\n",
    "    \n",
    "    # Adjust layout and show figure\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Return dictionary of classification_report\n",
    "    if output_dict==True:\n",
    "        report_dict = classification_report(y_true, y_pred, output_dict=True)\n",
    "        return report_dict\n",
    "\n",
    "\n",
    "\n",
    "## PLOT_HISTORY FUNCTION FROM WEEK 3\n",
    "def plot_history(history,figsize=(6,8)):\n",
    "    # Get a unique list of metrics \n",
    "    all_metrics = np.unique([k.replace('val_','') for k in history.history.keys()])\n",
    "\n",
    "    # Plot each metric\n",
    "    n_plots = len(all_metrics)\n",
    "    fig, axes = plt.subplots(nrows=n_plots, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Loop through metric names add get an index for the axes\n",
    "    for i, metric in enumerate(all_metrics):\n",
    "\n",
    "        # Get the epochs and metric values\n",
    "        epochs = history.epoch\n",
    "        score = history.history[metric]\n",
    "\n",
    "        # Plot the training results\n",
    "        axes[i].plot(epochs, score, label=metric, marker='.')\n",
    "        # Plot val results (if they exist)\n",
    "        try:\n",
    "            val_score = history.history[f\"val_{metric}\"]\n",
    "            axes[i].plot(epochs, val_score, label=f\"val_{metric}\",marker='.')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        finally:\n",
    "            axes[i].legend()\n",
    "            axes[i].set(title=metric, xlabel=\"Epoch\",ylabel=metric)\n",
    "\n",
    "    # Adjust subplots and show\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "def evaluate_classification_network(model, \n",
    "                                    X_train=None, y_train=None, \n",
    "                                    X_test=None, y_test=None,\n",
    "                                    history=None, history_figsize=(6,6),\n",
    "                                    figsize=(6,4), normalize='true',\n",
    "                                    output_dict = False,\n",
    "                                    cmap_train='Blues',\n",
    "                                    cmap_test=\"Reds\",\n",
    "                                    values_format=\".2f\", \n",
    "                                    colorbar=False):\n",
    "    \"\"\"Evaluates a neural network classification task using either\n",
    "    separate X and y arrays or a tensorflow Dataset\n",
    "    \n",
    "    Data Args:\n",
    "        X_train (array, or Dataset)\n",
    "        y_train (array, or None if using a Dataset\n",
    "        X_test (array, or Dataset)\n",
    "        y_test (array, or None if using a Dataset)\n",
    "        history (history object)\n",
    "        \"\"\"\n",
    "    # Plot history, if provided\n",
    "    if history is not None:\n",
    "        plot_history(history, figsize=history_figsize)\n",
    "    ## Adding a Print Header\n",
    "    print(\"\\n\"+'='*80)\n",
    "    print('- Evaluating Network...')\n",
    "    print('='*80)\n",
    "    ## TRAINING DATA EVALUATION\n",
    "    # check if X_train was provided\n",
    "    if X_train is not None:\n",
    "        ## Check if X_train is a dataset\n",
    "        if hasattr(X_train,'map'):\n",
    "            # If it IS a Datset:\n",
    "            # extract y_train and y_train_pred with helper function\n",
    "            y_train, y_train_pred = get_true_pred_labels(model, X_train)\n",
    "        else:\n",
    "            # Get predictions for training data\n",
    "            y_train_pred = model.predict(X_train)\n",
    "        ## Pass both y-vars through helper compatibility function\n",
    "        y_train = convert_y_to_sklearn_classes(y_train)\n",
    "        y_train_pred = convert_y_to_sklearn_classes(y_train_pred)\n",
    "        \n",
    "        # Call the helper function to obtain regression metrics for training data\n",
    "        results_train = classification_metrics(y_train, y_train_pred, \n",
    "                                         output_dict=True, figsize=figsize,\n",
    "                                             colorbar=colorbar, cmap=cmap_train,\n",
    "                                               values_format=values_format,\n",
    "                                         label='Training Data')\n",
    "        \n",
    "        ## Run model.evaluate         \n",
    "        print(\"\\n- Evaluating Training Data:\")\n",
    "        print(model.evaluate(X_train, return_dict=True))\n",
    "    \n",
    "    # If no X_train, then save empty list for results_train\n",
    "    else:\n",
    "        results_train = []\n",
    "    ## TEST DATA EVALUATION\n",
    "    # check if X_test was provided\n",
    "    if X_test is not None:\n",
    "        ## Check if X_train is a dataset\n",
    "        if hasattr(X_test,'map'):\n",
    "            # If it IS a Datset:\n",
    "            # extract y_train and y_train_pred with helper function\n",
    "            y_test, y_test_pred = get_true_pred_labels(model, X_test)\n",
    "        else:\n",
    "            # Get predictions for training data\n",
    "            y_test_pred = model.predict(X_test)\n",
    "        ## Pass both y-vars through helper compatibility function\n",
    "        y_test = convert_y_to_sklearn_classes(y_test)\n",
    "        y_test_pred = convert_y_to_sklearn_classes(y_test_pred)\n",
    "        \n",
    "        # Call the helper function to obtain regression metrics for training data\n",
    "        results_test = classification_metrics(y_test, y_test_pred, \n",
    "                                         output_dict=True, figsize=figsize,\n",
    "                                             colorbar=colorbar, cmap=cmap_test,\n",
    "                                              values_format=values_format,\n",
    "                                         label='Test Data')\n",
    "        \n",
    "        ## Run model.evaluate         \n",
    "        print(\"\\n- Evaluating Test Data:\")\n",
    "        print(model.evaluate(X_test, return_dict=True))\n",
    "      \n",
    "    # If no X_test, then save empty list for results_test\n",
    "    else:\n",
    "        results_test = []\n",
    "      \n",
    "    # Store results in a dictionary\n",
    "    results_dict = {'train':results_train,\n",
    "                    'test': results_test}\n",
    "    if output_dict == True:\n",
    "        return results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf9fa00-002e-4fe2-b92c-973908168722",
   "metadata": {},
   "source": [
    "# <u>Making the TensorFlow Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64b0ea31-818a-4712-92db-353a0a086784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data/xray/'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the contents of dataset folder\n",
    "data_dir = 'Data/xray/'\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdec9b37-b0a2-4b58-9c05-7eaf58bf3234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints', 'covid', 'normal', 'pneumonia']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the list of folders in the data_dir\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dfd7f22-5061-44a0-b331-037bb321ddec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4938"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting list of all img file paths\n",
    "img_files = glob.glob(data_dir+\"**/*\")\n",
    "len(img_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee1c79e5-6ca7-4c20-8ba6-161dca05e534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving image params as vars for reuse\n",
    "batch_size = 32\n",
    "img_height = 96\n",
    "img_width = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55b009f9-9b08-4eaf-84ea-bb8a2c605ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4902 files belonging to 4 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=(TensorSpec(shape=(None, 96, 96, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 4), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make the dataset from the main folder of images\n",
    "ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "    shuffle=True,\n",
    "    label_mode='categorical',\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fda12604-2bc2-4534-b5ea-ac0e3de06ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine number of batches in dataset\n",
    "ds_size = len(ds)\n",
    "ds_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b74f791-b55d-4c0c-8c4e-47ac0471d202",
   "metadata": {},
   "source": [
    "## `Saving Class Info`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3799111c-7ca8-484c-a85c-854f2c738836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints', 'covid', 'normal', 'pneumonia']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the class names for later use\n",
    "class_names = ds.class_names\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2696a190-b1d4-4518-92a0-2248f79856a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving # of classes for later use\n",
    "num_classes = len(class_names)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a98b3ed-e574-40eb-ab65-0a2d8e7ba036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '.ipynb_checkpoints', 1: 'covid', 2: 'normal', 3: 'pneumonia'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving dictionary of integer:string labels\n",
    "class_dict = dict(zip(range(num_classes), class_names))\n",
    "class_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402b26f1-c838-4242-b0e3-511a35f63296",
   "metadata": {},
   "source": [
    "## `Split the dataset into a training-validation-test split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eabb7142-7120-4912-9da7-037fd9d379f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use 107 batches as training data\n",
      "Use 30 batches as validation data\n",
      "The remaining 17 batches will be used as test data.\n"
     ]
    }
   ],
   "source": [
    "# Set the ratio of the train, validation, test split\n",
    "split_train = 0.7\n",
    "split_val = 0.2\n",
    "split_test = .1 \n",
    "# Calculate the number of batches for training and validation data \n",
    "n_train_batches =  int(ds_size * split_train)\n",
    "n_val_batches = int(ds_size * split_val)\n",
    "print(f\"Use {n_train_batches} batches as training data\")\n",
    "print(f\"Use {n_val_batches} batches as validation data\")\n",
    "print(f\"The remaining {len(ds)- (n_train_batches+n_val_batches)} batches will be used as test data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1847b3f8-5180-4ad2-8aff-382625b4a67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set is 107 batches long.\n"
     ]
    }
   ],
   "source": [
    "# Use .take to slice out the number of batches \n",
    "train_ds = ds.take(n_train_batches)\n",
    "# Confirm the length of the training set\n",
    "print(f'The training set is {len(train_ds)} batches long.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b41978de-5c17-4b61-b554-f88b84aa0e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation set is 30 batches long.\n"
     ]
    }
   ],
   "source": [
    "# Skipover the training batches\n",
    "val_ds = ds.skip(n_train_batches)\n",
    "# Take the correct number of validation batches\n",
    "val_ds = val_ds.take(n_val_batches)\n",
    "# Confirm the length of the validation set\n",
    "print(f'The validation set is {len(val_ds)} batches long.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28fb00dc-b578-499f-8491-57b911c11622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing set is 17 batches long.\n"
     ]
    }
   ],
   "source": [
    "# Skip over all of the training + validation batches\n",
    "test_ds = ds.skip(n_train_batches + n_val_batches)\n",
    "# Confirm the length of the testing data\n",
    "print(f'The testing set is {len(test_ds)} batches long.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b7d8aa-c1e1-406c-9a46-1f8c63223731",
   "metadata": {},
   "source": [
    "# <u> `Optimize the Dataset`\n",
    "- ## Add a shuffle step to the training dataset\n",
    "- ## Add caching and prefetching to all 3 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb8093ff-3dfd-4726-8bc0-7af197c03970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f3e7e1c-4430-418c-b7e4-123a040083b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use autotune to automatically determine best buffer sizes \n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Optimize training data\n",
    "train_ds = train_ds.cache().shuffle(buffer_size= len(train_ds),\n",
    "                                   seed=42).prefetch(buffer_size=AUTOTUNE)\n",
    "# Optimize validation data\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "# Optimize teset data\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3236e3-1d52-45e1-a310-c7dfe7dca0e2",
   "metadata": {},
   "source": [
    "#  `Preview the Data and Save the Shape:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f03e2c9f-647e-4de9-8292-e0a1c2486f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 96, 96, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get image sizes for later use\n",
    "example_batch_imgs,example_batch_y= ds.take(1).get_single_element()\n",
    "example_batch_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a1af59f-1ebc-472a-a120-d76dc6ab67c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([96, 96, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Individual image shape\n",
    "input_shape = example_batch_imgs[0].shape\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6125dd3-99e0-48eb-ad52-fc3ceb12112b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=96x96>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview an image from the Dataset\n",
    "array_to_img(example_batch_imgs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d483207-83cc-45b1-be2a-7d20ae5289cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'covid'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show label value\n",
    "label = class_dict[np.argmax(example_batch_y[0])]\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb74ce42-6ea7-4019-b1e0-1cae6a25b9ef",
   "metadata": {},
   "source": [
    "# `Modeling`\n",
    "## <u>Modeling Steps (for every model):\n",
    "- ### **For every model**\n",
    "    - ### Fitting the Model\n",
    "        - using validation_data\n",
    "        - many epochs (20+)\n",
    "        - an EarlyStopping callback\n",
    "        - Save the training history \n",
    "- ### **Evaluating the Model:**\n",
    "    - ### Plot the training history\n",
    "    - ### Evaluating the model on the training and test data, including:\n",
    "        - Sckit-learning confusion matrix\n",
    "        - Sckit-learning classification Report\n",
    "        - The results from model.evaluate method\n",
    "            - (`Tip`: Use your custom flexible evaluation functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5481679c-5539-4a31-b4f0-9d6bb51c9f7c",
   "metadata": {},
   "source": [
    "## `1) Build a Simple CNN Model`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "427c53fe-0a41-45dd-a50f-0e939904eade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get early stopping function that can be reused on all models\n",
    "def get_callbacks(patience=3, monitor='val_accuracy'):\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(patience=patience, monitor=monitor)\n",
    "    return [early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f728334-aea8-4234-b7d5-79f61472596b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the building and compiling steps within a function\n",
    "def build_model():\n",
    "    # Instantatie model\n",
    "    model = models.Sequential()\n",
    "    # Scaling layer\n",
    "    scaling_layer = layers.Rescaling(1./255, input_shape=input_shape)\n",
    "    model.add(scaling_layer)\n",
    "    \n",
    "    # Convolutional layer\n",
    "    model.add(layers.Conv2D(filters=8, kernel_size=3, input_shape=input_shape, padding='same')) \n",
    "    # Pooling layer\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))\n",
    "    \n",
    "    # Convolutional layer\n",
    "    model.add(layers.Conv2D(filters=8, kernel_size=3, input_shape=input_shape, padding='same')) \n",
    "    # Pooling layer\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))\n",
    "    \n",
    "    # Flattening layer\n",
    "    model.add(layers.Flatten())\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(3, activation=\"softmax\")) \n",
    "\n",
    "    # Compile\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5a230f8-fe4b-40c8-a3f8-bb5dbf9e1911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling (Rescaling)       (None, 96, 96, 3)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 96, 96, 8)         224       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 48, 48, 8)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 48, 48, 8)         584       \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 24, 24, 8)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 13827     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,635\n",
      "Trainable params: 14,635\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model1 = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efe1d92b-c266-47c7-9c78-766934cb30f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Valde\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Valde\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Valde\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Valde\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\Valde\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\keras\\engine\\training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\Valde\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\Valde\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\keras\\losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\Valde\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\keras\\losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\Valde\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\keras\\losses.py\", line 1984, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\Valde\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\keras\\backend.py\", line 5559, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 4) and (None, 3) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_callbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filevbck6bp1.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Valde\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Valde\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Valde\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Valde\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\Valde\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\keras\\engine\\training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\Valde\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\Valde\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\keras\\losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\Valde\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\keras\\losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\Valde\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\keras\\losses.py\", line 1984, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\Valde\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\keras\\backend.py\", line 5559, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 4) and (None, 3) are incompatible\n"
     ]
    }
   ],
   "source": [
    "history = model1.fit(\n",
    "    train_ds,\n",
    "    validation_data = val_ds,\n",
    "    epochs=25,\n",
    "    callbacks=get_callbacks(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1076100f-41de-4919-92b0-c9808c22446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with the CNN + Dataset\n",
    "evaluate_classification_network(model1,\n",
    "                                X_train=train_ds,\n",
    "                                X_test=test_ds,\n",
    "                                figsize=(9,9),\n",
    "                                values_format='0.1f',\n",
    "                                history=history);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1dddff-1a31-4990-a348-f0aaba0191a6",
   "metadata": {},
   "source": [
    "**`Evaluation`**\n",
    "- The model reached 89% accuracy, which will be a good starting point for further optimization/tuning.\n",
    "- To try and further increase performance, we will use a Dense layer, additional Conv2D layers, and try a higher number of filters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa91d35d-5be1-4973-b9d8-cfd9d0945f7f",
   "metadata": {},
   "source": [
    "## `2) Build a more complex CNN Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0785ad02-ab3d-4f30-a55e-f40b69c7672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the building and compiling steps within a function\n",
    "def build_model():\n",
    "    # Instantatie model\n",
    "    model = models.Sequential()\n",
    "    # Scaling layer\n",
    "    model.add(layers.Rescaling(1./255, input_shape=input_shape))\n",
    "    \n",
    "    # Convolutional layer - additional filters\n",
    "    model.add(layers.Conv2D(filters=16, kernel_size=3, input_shape=input_shape, padding='same')) \n",
    "    # Pooling layer\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))\n",
    "    \n",
    "    # Convolutional layer - additional Conv2D\n",
    "    model.add(layers.Conv2D(filters=16, kernel_size=3, input_shape=input_shape, padding='same')) \n",
    "    model.add(layers.Conv2D(filters=16, kernel_size=3, input_shape=input_shape, padding='same')) \n",
    "    # Pooling layer\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))\n",
    "\n",
    "    # Hidden dense layer\n",
    "    model.add(layers.Dense(32, activation=\"relu\"))\n",
    "    \n",
    "    # Flattening layer\n",
    "    model.add(layers.Flatten())\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(3, activation=\"softmax\")) \n",
    "\n",
    "    # Compile\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d368cda-cbb7-450e-9a01-a50a4a82e52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model_2 = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab66ce01-b334-46ea-a4a6-21aa55cf050b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain and save history for examination\n",
    "history_2 = model_2.fit(\n",
    "    train_ds,\n",
    "    validation_data = val_ds,\n",
    "    epochs=25,\n",
    "    callbacks=get_callbacks(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfa212f-10a8-4777-ad9e-8fd17bb1bd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with the CNN + Dataset\n",
    "evaluate_classification_network(model_2,\n",
    "                                X_train=train_ds,\n",
    "                                X_test=test_ds,\n",
    "                                figsize=(9,9),\n",
    "                                values_format='0.1f',\n",
    "                                history=history_2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a2fba6-154a-4bec-b619-80890802b568",
   "metadata": {},
   "source": [
    "**`Evaluation`**\n",
    "- The model has now gone to just over 91% accuracy, up from 89%. It seems the combined increase in filters, extra Conv2D, and dense layers helped the model slightly.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4d1100-9122-4cb0-b028-fae2e21eca28",
   "metadata": {},
   "source": [
    "## `3) Build a Transfer Learning Model using a Keras Application`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5c75a9-7c8d-4c2c-96fa-d9a205fba21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download base model\n",
    "inception_base= tf.keras.applications.InceptionV3(include_top=False, input_shape=input_shape)\n",
    "\n",
    "# Make it not trainable\n",
    "inception_base.trainable=False\n",
    "vk.layered_view(inception_base, legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347d900f-09f3-4f7f-a0f7-167582e3f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add preprocessing lambda layer\n",
    "lambda_layer_inception = tf.keras.layers.Lambda(tf.keras.applications.inception_v3.preprocess_input, \n",
    "                                      name='preprocess_input_inceptv3')\n",
    "\n",
    "\n",
    "def build_inception_model():\n",
    "    model = models.Sequential(name=\"InceptionV3\")\n",
    "    # Use input layer (lambda layer will handle rescaling).\n",
    "    model.add(tf.keras.layers.Input(shape=input_shape))\n",
    "\n",
    "    ## Adding preprocessing lamabda layer\n",
    "    model.add(lambda_layer_inception)\n",
    "\n",
    "    # Add pretrained base\n",
    "    model.add(inception_base)\n",
    "\n",
    "    # Flattening layer\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    ## Adding a Hidden Dense Layer\n",
    "    model.add(layers.Dense(256, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(len(class_names), activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dffcf43-6df3-4f90-be67-e8e15c3c1691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build, fit, and evaluate EfficientNet Model\n",
    "model_inception = build_inception_model()\n",
    "history = model_inception.fit(train_ds, validation_data=val_ds,epochs=20, \n",
    "                    callbacks=get_callbacks()\n",
    "                   )\n",
    "evaluate_classification_network(model_inception, X_test=test_ds, history=history);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d28094-1ed8-4cf6-a70b-57dc8f28d9bb",
   "metadata": {},
   "source": [
    "**`Evaluation`**\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4f551a-ac65-472b-9ef0-278887b71c21",
   "metadata": {},
   "source": [
    "## **`Choosing the Best Model`**\n",
    "- ### For the best model, I would go with InceptionV3. Accuracy was similar to the baseline and slightly below the adjusted model, but the loss score was better. While the first two models were slightly faster to get close to the same result, the Inception model looks like it may be overfitting less and may improve it's scores if allowed to run longer or with larger images/more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2632ee00-be80-4ad4-866a-8abd6d5d5eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model for later use\n",
    "# Select the best model\n",
    "best_model = model_inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b8161f-15e2-44cf-9d5d-b4c80f04d251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make folder for model\n",
    "import os\n",
    "folder = 'BestModels/'\n",
    "os.makedirs(folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cc4b0d-7950-4b54-83e8-ccb27f0c6e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the best model\n",
    "model_fname = 'BestModels/best_lung_class_model.keras'\n",
    "best_model.save(model_fname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
